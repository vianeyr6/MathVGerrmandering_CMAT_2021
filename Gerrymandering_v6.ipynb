{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accessible-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery-storage in /opt/conda/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (0.3.18)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (1.26.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (20.9)\n",
      "Requirement already satisfied: proto-plus>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (3.15.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (49.6.0.post20210108)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.26.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.15.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2021.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.32.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.7.2)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (3.7.4.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "straight-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cell defines the Gerry class which does all the work\n",
    "# I don't think you will need to edit this cell at all\n",
    "proj_id = 'cmat-315920'\n",
    "root_path = '/home/jupyter'\n",
    "\n",
    "import google, pathlib, shutil, time, datetime, dataclasses, typing, numpy as np, pandas as pd, geopandas as gpd, networkx as nx\n",
    "import matplotlib.pyplot as plt, plotly.express as px \n",
    "from shapely.ops import orient\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient, types\n",
    "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "cred, proj = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "bqclient = bigquery.Client(credentials = cred, project = proj)\n",
    "crs_map = 'NAD83'\n",
    "crs_area = 'ESRI:102003'\n",
    "crs_length = 'ESRI:102005'\n",
    "# input is WKT in NAD83 - https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2020/TGRSHP2020_TechDoc_Ch3.pdf\n",
    "# use ESRI:102003 for area calculations - https://epsg.io/102003\n",
    "# use ESRI:102005 for length calculations - https://epsg.io/102005\n",
    "\n",
    "def get_states():\n",
    "    qry = f\"\"\"\n",
    "    select\n",
    "        state_fips_code as fips\n",
    "        , state_postal_abbreviation as abbr\n",
    "        , state_name as name\n",
    "    from\n",
    "        bigquery-public-data.census_utility.fips_codes_states\n",
    "    \"\"\"\n",
    "    return bqclient.query(qry).result().to_dataframe()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Gerry:\n",
    "    # These are default values that can be overridden when you create the object\n",
    "    abbr              : str\n",
    "    yr                : int\n",
    "    geo_simplification: float = 0.003\n",
    "    min_graph_degree  : int = 1\n",
    "    pop_err_max_pct   : float = 2.0\n",
    "    seed              : int = 42\n",
    "    overwrite         : typing.Any = False\n",
    "    clr_seq           : typing.Any = tuple(px.colors.qualitative.Antique)\n",
    "    # px.colors.qualitative.swatches() # shows available color schemes\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self.__dict__[key] = val\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self['rng'] = np.random.default_rng(self['seed'])\n",
    "        self['congress'] = int((self['yr']-1786)/2)\n",
    "        self['races'] = ['total', 'white', 'black', 'asian', 'hispanic', 'amerindian', 'other_race', 'two_or_more_races']\n",
    "        self['race_pops'] = [f'{r}_pop' for r in self['races']]\n",
    "        I = [10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 60000, 75000, 100000, 125000, 150000, 200000]\n",
    "        self['income_levels'] = [f'income_less_{I[0]}'] + [f'income_{I[j]}_{I[j+1]-1}' for j in range(len(I)-1)] + [f'income_{I[-1]}_or_more']\n",
    "        def rgb_to_hex(c):\n",
    "            if c[0] == '#':\n",
    "                return c\n",
    "            else:\n",
    "                return '#%02x%02x%02x' % tuple(int(rgb) for rgb in c[4:-1].split(', '))\n",
    "        self['clr_seq'] = [rgb_to_hex(c) for c in self['clr_seq']]\n",
    "        \n",
    "        self.__dict__.update(states[states['abbr']==self['abbr']].iloc[0])\n",
    "        self['run_path'] = pathlib.Path(f'{root_path}/simulations/{self[\"yr\"]}/{self[\"abbr\"]}')\n",
    "        self['run_path'].mkdir(parents=True, exist_ok=True)\n",
    "        self['files'] = {'bgs'  : self['run_path'] / 'bgs.parquet',\n",
    "                         'pairs': self['run_path'] / 'pairs.parquet',\n",
    "                         'graph': self['run_path'] / 'graph.gpickle',\n",
    "                        }\n",
    "        if self['overwrite'] is True or str(self['overwrite']).lower() == 'all':\n",
    "            O = self['files'].keys()\n",
    "        elif self['overwrite'] is False or self['overwrite'] is None or self['overwrite'] == '' or self['overwrite'] == []:\n",
    "            O = []\n",
    "        else:\n",
    "            O = self['overwrite']\n",
    "        for key in O:\n",
    "            try:\n",
    "                f = self['files'][key]\n",
    "                f.unlink()\n",
    "                print(f'unlinked {f}')\n",
    "            except:\n",
    "                print(f'No file associated to \"{key}\" found')\n",
    "\n",
    "        self.get_bgs()\n",
    "        return\n",
    "        self['bgs'].insert(3, 'step', 0)\n",
    "        self['total_pop']  = self['bgs']['total_pop'].sum()\n",
    "        self['cd_names'] = np.unique(self['bgs']['cd'])\n",
    "        self['cd_count'] = len(self.cd_names)\n",
    "        self['pop_target'] = self.total_pop / self.cd_count\n",
    "        self['pop_err_max'] =  self.total_pop * self.pop_err_max_pct / 100\n",
    "        self.get_bgs()\n",
    "        self.get_pairs()\n",
    "        self['transit_denom'] = 100\n",
    "        self.compute_stats()\n",
    "        self['transit_denom'] = self['stats']['cd_transit'].sum()\n",
    "\n",
    "    def set_dtypes(self, df):\n",
    "        # does NOT work inplace\n",
    "        dtypes = {'geo_id':str, 'step':np.uint16,\n",
    "                  'state_fips':np.uint8, 'county_fips':np.uint8,'tract_ce':np.uint32, 'blockgroup_ce':np.uint8,\n",
    "                  'cd_orig':np.uint8, 'cd':np.uint8,\n",
    "                  'lon':np.float64, 'lat':np.float64, 'distance':np.float64,\n",
    "                  'total_pop':np.uint32, 'cd_pop':np.uint32,\n",
    "                  'area':np.float64, 'cd_area':np.float64,\n",
    "                  'perim':np.float64, 'cd_perim':np.float64, 'perim_shared':np.float64, 'touch':bool,\n",
    "                  'cd_polsby':np.float64, 'transit':np.float64, 'cd_transit':np.float64,\n",
    "                 }\n",
    "        for V in ['race_pops', 'income_levels']:\n",
    "            dtypes.update({x:np.uint32 for x in self[V]})\n",
    "            dtypes.update({f'cd_{x}':np.uint32 for x in self[V]})\n",
    "        return df.astype({c:d for c,d in dtypes.items() if c in df.columns}, copy=False)\n",
    "\n",
    "    def to_crs(self, df, crs):\n",
    "        # works inplace\n",
    "        df.to_crs(crs=crs, inplace=True)\n",
    "        for c in df.columns:\n",
    "            if c[:8] in ['geometry', 'centroid']:\n",
    "                df[c] = df[c].to_crs(crs=crs)\n",
    "        return df\n",
    "\n",
    "    def read_data(self, variable):\n",
    "        \"\"\"Check if the data already exists so we can reuse it without pulling it again\"\"\"\n",
    "        try:\n",
    "            # Does the object already have it?\n",
    "            self[variable]\n",
    "        except:\n",
    "            # If not, is it stored in a local file?\n",
    "            print(f'Getting {variable} - ', end='')\n",
    "            f = self['files'][variable]\n",
    "            try:\n",
    "                self[variable] = gpd.read_parquet(f)\n",
    "                print(f'found {f}')\n",
    "            except:\n",
    "                try:\n",
    "                    self[variable] = pd.read_parquet(f)\n",
    "                    print(f'found {f}')\n",
    "                except:\n",
    "                    try:\n",
    "                        self[variable] = nx.read_gpickle(f)\n",
    "                        print(f'found {f}')\n",
    "                    except:\n",
    "                        # Nope, there's no file with that data ... gotta go get it\n",
    "                        print('no local file found - compiling from source')\n",
    "                        return False\n",
    "        return True\n",
    "    \n",
    "    def find_closest(self, point, others):\n",
    "        d = others.distance(point)\n",
    "        return others.loc[d.idxmin()]\n",
    "\n",
    "    def get_bgs(self):\n",
    "        if self.read_data('bgs'):\n",
    "            self['bgs'] = self.set_dtypes(self.to_crs(self['bgs'], crs_map))\n",
    "        else:\n",
    "            qry = f\"\"\"\n",
    "            select\n",
    "                --geo_id structure - https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
    "                *\n",
    "            from (\n",
    "                select\n",
    "                    *\n",
    "                from (\n",
    "                    -- get shapes demographic data\n",
    "                    select distinct\n",
    "                        geo_id as geo_id\n",
    "                        , cast(substring(geo_id, 0 , 2) as int) as state_fips\n",
    "                        , cast(substring(geo_id, 3 , 3) as int) as county_fips\n",
    "                        , cast(substring(geo_id, 5 , 6) as int) as tract_ce\n",
    "                        , cast(substring(geo_id, 12, 1) as int) as blockgroup_ce\n",
    "                        , {\", \".join(self['race_pops'])}\n",
    "                        , {\", \".join(self['income_levels'])}\n",
    "                    from\n",
    "                        bigquery-public-data.census_bureau_acs.blockgroup_{self['yr']}_5yr\n",
    "                    where\n",
    "                        left(geo_id, 2) = \"{self['fips']}\"\n",
    "                    ) as acs\n",
    "                full outer join (\n",
    "                    -- get shapes\n",
    "                    select\n",
    "                        geo_id as geo_id_geo\n",
    "                        , blockgroup_geom as geometry\n",
    "                    from\n",
    "                        bigquery-public-data.geo_census_blockgroups.blockgroups_{self['fips']}\n",
    "                    ) as geo\n",
    "                on\n",
    "                    acs.geo_id = geo.geo_id_geo\n",
    "                ) as acs_geo\n",
    "            full outer join (\n",
    "                select\n",
    "                    *\n",
    "                from (\n",
    "                    -- get population weighted centroids\n",
    "                    -- must build geo_id because data source does not include it\n",
    "                    select distinct\n",
    "                        concat( \n",
    "                            lpad(cast(STATEFP as string), 2, \"0\"),\n",
    "                            lpad(cast(COUNTYFP as string), 3, \"0\"),\n",
    "                            lpad(cast(TRACTCE as string), 6, \"0\"),\n",
    "                            lpad(cast(BLKGRPCE as string), 1, \"0\")\n",
    "                            ) as geo_id_centroids\n",
    "                        , LONGITUDE as lon\n",
    "                        , LATITUDE as lat\n",
    "                    from\n",
    "                        {proj_id}.BLOCKGROUP_CENTROIDS.blockgroup_centroids_{self['fips']}\n",
    "                    ) as centroids\n",
    "                full outer join (\n",
    "                    -- get congressional district\n",
    "                    -- at block level -> must aggregate to blockgroup\n",
    "                    -- 7141 (3%) of blockgroups span multiple congressional districts\n",
    "                    -- We assign that entire bg to the cd with the most blocks\n",
    "                    select\n",
    "                        geo_id_cd\n",
    "                        , cd\n",
    "                    from (\n",
    "                        select\n",
    "                            A.*\n",
    "                            , rank() over (partition by A.geo_id_cd order by A.num_blocks_in_cd desc) as r\n",
    "                        from (\n",
    "                            select\n",
    "                                left(BLOCKID, 12) as geo_id_cd   -- remove last 4 char to get blockgroup geo_id\n",
    "                                , CD{self['congress']} as cd\n",
    "                                , count(*) as num_blocks_in_cd\n",
    "                            from \n",
    "                                {proj_id}.Block_Equivalency_Files.{self['congress']}th_BEF\n",
    "                            where\n",
    "                                left(blockid, 2) = \"{self['fips']}\"\n",
    "                            group by\n",
    "                                1, 2\n",
    "                            ) as A\n",
    "                        ) as B\n",
    "                    where\n",
    "                        r = 1\n",
    "                    ) as cd\n",
    "                on\n",
    "                    centroids.geo_id_centroids = cd.geo_id_cd\n",
    "                ) as centroids_cd\n",
    "            on acs_geo.geo_id = centroids_cd.geo_id_cd\n",
    "            \"\"\"\n",
    "            bgs = bqclient.query(qry).result().to_dataframe()\n",
    "            bgs['geometry'] = gpd.GeoSeries.from_wkt(bgs['geometry']).apply(lambda p: orient(p, -1))\n",
    "            bgs['centroid'] = gpd.points_from_xy(bgs['lon'], bgs['lat'], crs=crs_map)\n",
    "            bgs = gpd.GeoDataFrame(bgs, geometry='geometry', crs=crs_map)\n",
    "#             bgs['area']  = self.to_crs(bgs, crs_area).area / 1000 / 1000\n",
    "#             bgs['perim'] = self.to_crs(bgs, crs_length).length / 1000\n",
    "            \n",
    "            self.to_crs(bgs, crs_length)\n",
    "            \n",
    "            # fix mismatches\n",
    "            cd_row_mask =   bgs['geo_id'].isnull() & ~bgs['geo_id_cd'].isnull()\n",
    "            acs_row_mask = ~bgs['geo_id'].isnull() &  bgs['geo_id_cd'].isnull()\n",
    "            if acs_row_mask.any() or cd_row_mask.any():\n",
    "                bgs_matched = bgs[~acs_row_mask & ~cd_row_mask]\n",
    "                cd_col_mask  = ~bgs[cd_row_mask] .isnull().any(axis=0)\n",
    "                acs_col_mask = ~bgs[acs_row_mask].isnull().any(axis=0)\n",
    "                acs_col_mask['centroid'] = False\n",
    "                A = bgs.loc[acs_row_mask, acs_col_mask]\n",
    "                B = bgs.loc[cd_row_mask , cd_col_mask ]\n",
    "                C = A.merge(B, how='cross')\n",
    "                C['match'] = C['geometry'].contains(C['centroid'])\n",
    "                def f(X):\n",
    "                    N = X['match'].sum()\n",
    "                    if N != 1:\n",
    "                        X['centroid'] = X['geometry'].centroid\n",
    "                        X['cd'] = np.nan  # fill later\n",
    "                    return X.nlargest(1, columns='match').drop(columns='match')\n",
    "                bgs_unmatched = C.groupby('geo_id').apply(f)\n",
    "                bgs = pd.concat([bgs_matched, bgs_unmatched], ignore_index=True)\n",
    "        \n",
    "            cd_qual = (bgs.groupby('cd')['total_pop'].transform('sum')) > 0\n",
    "            others = bgs[cd_qual]\n",
    "            for i, bg in bgs[~cd_qual].iterrows():\n",
    "                nbr = self.find_closest(bg['centroid'], others)\n",
    "                print(f\"Given cd for {bg['geo_id']} is {bg['cd']} ... changing to {nbr['cd']} based on nearest neighbor {nbr['geo_id']}\")\n",
    "                bgs.loc[i, 'cd'] = nbr['cd']\n",
    "            \n",
    "            t = bgs.pop('cd')\n",
    "            bgs.insert(1, 'cd_orig', t.copy())\n",
    "            bgs.insert(2, 'cd', t.copy())\n",
    "            self['bgs'] = self.set_dtypes(bgs.drop(columns=['geo_id_geo', 'geo_id_cd', 'geo_id_centroids']).sort_values(['cd', 'geo_id']))\n",
    "#             display(self['bgs'].groupby('cd')['total_pop'].agg(['count','sum']).sort_index())\n",
    "            mask = self['bgs'].isnull().any(axis=1)\n",
    "            assert ~mask.any(), f\"Found null values\\n{self['bgs'][mask]}\"\n",
    "            self['bgs'].to_parquet(self['files']['bgs'], index=False)\n",
    "            \n",
    "            return\n",
    "            self.get_pairs()\n",
    "            self.to_crs(self['bgs'], crs_map)\n",
    "            self['bgs']['geometry'] = self['bgs']['geometry'].simplify(self['geo_simplification'])\n",
    "            self['bgs'].to_parquet(self['files']['bgs'], index=False)\n",
    "\n",
    "    def get_pairs(self):\n",
    "        if self.read_data('pairs'):\n",
    "            self['pairs'] = self.set_dtypes(self['pairs'])\n",
    "        else:\n",
    "            cols = ['geo_id', 'geometry', 'centroid']\n",
    "            df = self.to_crs(self['bgs'], crs_length)[cols].copy()\n",
    "            print(0)\n",
    "            pairs = df.merge(df, how='cross').query('geo_id_x < geo_id_y')\n",
    "            print(1)\n",
    "            pairs['distance']     = pairs.set_geometry('centroid_x').distance(    pairs.set_geometry('centroid_y'), align=False) / 1000\n",
    "            print(2)\n",
    "            pairs['perim_shared'] = pairs.set_geometry('geometry_x').intersection(pairs.set_geometry('geometry_y'), align=False).length / 1000\n",
    "            print(3)\n",
    "            pairs['touch'] = pairs['perim_shared'] > 1e-4\n",
    "            print(4)\n",
    "            pairs['transit'] = pairs['distance'] / 1341 * self['rng'].uniform(0.5, 1.5)  # 50 mph → 1341 m/min\n",
    "            print(5)\n",
    "            self['pairs'] = self.set_dtypes(pairs.drop(columns=[c+z for c in cols[1:] for z in ['_x', '_y']]))\n",
    "            print(6)\n",
    "            self['pairs'].to_parquet(self['files']['pairs'], index=False)\n",
    "\n",
    "    def edges_to_graph(self, edges):\n",
    "        edge_attr=['transit']\n",
    "        return nx.from_pandas_edgelist(edges, source='geo_id_x', target='geo_id_y', edge_attr=edge_attr)\n",
    "\n",
    "    def get_graph(self):\n",
    "        if self.read_data('graph'):\n",
    "            pass\n",
    "        else:\n",
    "            edges = self['pairs'].query('touch')\n",
    "            self['graph'] = self.edges_to_graph(edges)\n",
    "            node_attr = ['cd', 'total_pop']\n",
    "            nx.set_node_attributes(self['graph'], self['bgs'].set_index('geo_id')[node_attr].to_dict('index'))\n",
    "\n",
    "            for cd, nodes in self['bgs'].groupby('cd')['geo_id']:\n",
    "                while True:\n",
    "                    H = self['graph'].subgraph(nodes)\n",
    "                    components = [list(c) for c in nx.connected_components(H)]\n",
    "                    if len(components) == 1:\n",
    "                        break\n",
    "                    print(f'CD {cd} has {len(components)} connected components ... adding edges to connect')\n",
    "                    qry = f'(geo_id_x in {components[0]} & geo_id_y in {components[1]}) | (geo_id_y in {components[0]} & geo_id_x in {components[1]})'\n",
    "                    cut_edges = self['pairs'].query(qry)\n",
    "                    edges = self['pairs'].query(f'distance == {cut_edges[\"distance\"].min()}')\n",
    "                    self['graph'].update(self.edges_to_graph(edges))\n",
    "\n",
    "            # ensure min degrees\n",
    "            edges = list()\n",
    "            for node, deg in self['graph'].degree:\n",
    "                k = self['min_graph_degree'] - deg\n",
    "                if k > 0:\n",
    "                    print(f'{node} has degree {deg} ... adding {k} more edge(s)')\n",
    "                    N = list(self['graph'].neighbors(node))\n",
    "                    df = self['pairs'].query(f'(geo_id_x == {node} & geo_id_y not in {N}) | (geo_id_y == {node} & geo_id_x not in {N})')\n",
    "                    edges.append(df.nsmallest(k, 'distance'))\n",
    "            if len(edges) > 0:\n",
    "                self['graph'].update(self.edges_to_graph(pd.concat(edges)))\n",
    "            nx.write_gpickle(self['graph'], self['files']['graph'])\n",
    "    \n",
    "    def get_cds(self):\n",
    "        return self['bgs'].groupby('cd')['geo_id'].apply(tuple).sort_index().to_dict()\n",
    "    \n",
    "    def get_hash(self):\n",
    "        return hash(tuple(self.get_cds().items()))\n",
    "\n",
    "    def compute_stats(self, step=None):\n",
    "        if step is not None:\n",
    "            self['bgs'] = self['bgs'].drop(columns=['cd','step']).merge(self['bgs_hist'].query(f'step == {step}'), on='geo_id')\n",
    "        def f(nodes):\n",
    "            n = nodes['geo_id'].tolist()\n",
    "            edges = self['pairs'].query(f'geo_id_x in {n} & geo_id_y in {n}')\n",
    "            s = {f'cd_{x}':nodes[x].sum() for x in ['area'] + self['race_pops'] + self['income_levels']}\n",
    "            s.update({'step'      : nodes['step'].max(),\n",
    "                      'cd_perim'  : nodes['perim'].sum() - 2 * edges['perim_shared'].sum(),\n",
    "                      'cd_transit': edges['transit'].sum() / self['transit_denom']  * 100,\n",
    "                     })\n",
    "            s.update({'cd_polsby' : (1 - 4 * np.pi * s['cd_area'] / (s['cd_perim']**2)) * 100})\n",
    "            return pd.Series(s)\n",
    "        stats = self['bgs'].groupby('cd').apply(f)\n",
    "        self['bgs'] = (self['bgs'].drop(columns=stats.columns, errors='ignore')\n",
    "                       .merge(stats, left_on='cd', right_index=True)\n",
    "                      )\n",
    "        self['bgs'] = self.set_dtypes(self['bgs'].sort_values('cd'))\n",
    "        self['stats'] = self.set_dtypes(stats.reset_index().sort_values('cd'))\n",
    "        return self['stats']\n",
    "    \n",
    "    def draw_map(self, step=None):\n",
    "        self.compute_stats(step)\n",
    "        step = 0 if step is None else step\n",
    "        df = self.to_crs(self['bgs'], crs_map).copy()\n",
    "        df['cd_total_pop'] = (df['cd_total_pop'] / self['total_pop'] * 100).round(1).astype(str)\n",
    "        df['cd_area'] = df['cd_area'].round(0).astype(int).astype(str)\n",
    "        df['cd_transit'] = df['cd_transit'].round(1).astype(str)\n",
    "        df['cd_polsby'] = df['cd_polsby'].round(1).astype(str)\n",
    "        df['cd'] = df['cd'].astype(str)\n",
    "        df['total_pop'] = df['total_pop'].round(0).astype(int).astype(str)\n",
    "        df['area'] = df['area'].round(0).astype(int).astype(str)\n",
    "        df['cd_stats'] = df['cd']+': pop='+df['cd_total_pop']+'%, area='+df['cd_area']+', tt='+df['cd_transit']+', pp='+df['cd_polsby']\n",
    "        fig = px.choropleth(df,\n",
    "                            geojson = df['geometry'],\n",
    "                            locations = df.index,\n",
    "                            color = \"cd_stats\",\n",
    "                            color_discrete_sequence = self['clr_seq'],\n",
    "                            hover_data = ['area', 'total_pop', 'lon', 'lat'],\n",
    "                           )\n",
    "        fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "        fig.update_layout({\n",
    "            'title'  : {'text':f'{self[\"name\"]} {self[\"yr\"]} step {step}', 'x':0.5, 'y':1.0},\n",
    "            'margin' : {'r':0, 't':20, 'l':0, 'b':0},\n",
    "            'legend' : {'y':0.5},\n",
    "        })\n",
    "        fig.show()\n",
    "\n",
    "    def draw_graph(self, step=None, layout=nx.spring_layout):\n",
    "        self.get_graph()\n",
    "        self.compute_stats(step)\n",
    "        pos = layout(self['graph'])\n",
    "        for cd, nodes in self['bgs'].groupby('cd')['geo_id']:\n",
    "            H = self['graph'].subgraph(nodes)\n",
    "            nx.draw_networkx_nodes(H, pos=pos, node_size=10, node_color=self['clr_seq'][cd-1])\n",
    "        nx.draw_networkx_edges(self['graph'], pos=pos)\n",
    "        plt.show()\n",
    "\n",
    "    def check_pop_balance(self, T):\n",
    "        comp = nx.connected_components(T)\n",
    "        next(comp)\n",
    "        s = sum(T.nodes[n]['total_pop'] for n in next(comp))\n",
    "        return abs(s - self.pop_target) <= self.pop_err_max\n",
    "        \n",
    "    def recom_step(self):\n",
    "        self.get_graph()\n",
    "        self['bgs'].set_index('geo_id', inplace=True)\n",
    "        recom_found = False\n",
    "        attempts = 0\n",
    "        for curr in self['rng'].permutation([(a,b) for a in self.cd_names for b in self.cd_names if a < b]).tolist():\n",
    "            nodes = self['bgs'].query(f'cd in {curr}').copy()\n",
    "            H = self['graph'].subgraph(nodes.index)\n",
    "            trees = []\n",
    "            for i in range(1000):\n",
    "                w = {e: self['rng'].uniform() for e in H.edges}\n",
    "                nx.set_edge_attributes(H, w, \"weight\")\n",
    "                T = nx.minimum_spanning_tree(H, \"weight\")\n",
    "                h = hash(tuple(sorted(T.edges)))\n",
    "                if h not in trees:\n",
    "                    trees.append(h)\n",
    "                    for e in self['rng'].permutation(T.edges):\n",
    "                        attempts += 1\n",
    "                        T.remove_edge(*e)\n",
    "                        if self.check_pop_balance(T):\n",
    "                            recom_found = True\n",
    "                            new = [list(c) for c in nx.connected_components(T)]\n",
    "                            nodes['cd_new'] = 0\n",
    "                            for n, c in zip(new, curr):\n",
    "                                nodes.loc[n, 'cd_new'] = c\n",
    "                            i = nodes.groupby(['cd','cd_new'])['area'].sum().idxmax()\n",
    "                            if i[0] != i[1]:\n",
    "                                new[0], new[1] = new[1], new[0]\n",
    "                            for n, c in zip(new, curr):\n",
    "                                self['bgs'].loc[n, 'cd'] = c\n",
    "                            break\n",
    "                        T.add_edge(*e)\n",
    "                else:\n",
    "                    print('Got a repeat spanning tree')\n",
    "                if recom_found:\n",
    "                    break\n",
    "            if recom_found:\n",
    "                break\n",
    "        self['bgs'].reset_index(inplace=True)\n",
    "        assert recom_found, \"No suitable recomb step found\"\n",
    "        return recom_found, attempts, trees\n",
    "        \n",
    "    def record(self, concat=False):\n",
    "        self.compute_stats()\n",
    "        record_cols = {'stats':['step','cd','cd_total_pop','cd_area','cd_polsby','cd_transit'],\n",
    "                       'bgs'  :['step','cd','geo_id']}\n",
    "        for X, cols in record_cols.items():\n",
    "            H = f'{X}_hist'\n",
    "            if concat:\n",
    "                self[H] = self.set_dtypes(pd.concat(self[H], ignore_index=True)[cols])\n",
    "                self[H].to_parquet(self['files']['run'] / f'{X}_hist.parquet', index=False)\n",
    "            else:\n",
    "                df = self.set_dtypes(self[X][cols])\n",
    "                try:\n",
    "                    self[H].append(df)\n",
    "                except:\n",
    "                    self[H] = [df]\n",
    "\n",
    "    def run_mcmc(self, steps=10, update_period=5):\n",
    "        def g(t):\n",
    "            hours, t = divmod(t, 60*60)\n",
    "            minutes, seconds = divmod(t, 60)\n",
    "            return f'{int(hours)}:{int(minutes)}:{seconds:.1f}'\n",
    "        \n",
    "        self.record()\n",
    "        start = time.perf_counter()\n",
    "        for step in range(1, steps+1):\n",
    "            self['bgs']['step'] = step\n",
    "            success, attempts, trees = self.recom_step()\n",
    "            self.record()\n",
    "            if step % update_period == 0:\n",
    "                stop = time.perf_counter()\n",
    "                elapsed = stop - start\n",
    "                total = elapsed / step * steps\n",
    "                remain = total - elapsed\n",
    "                print(f\"I've done {step} steps in {g(elapsed)}; time remaining {g(remain)} (est)\")\n",
    "\n",
    "        self['files']['run'] = self['run_path'] / f'runs/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}'\n",
    "        self['files']['run'].mkdir(parents=True, exist_ok=True)\n",
    "        self.record(concat=True)\n",
    "        self['steps'] = np.unique(self['bgs_hist']['step'])\n",
    "        \n",
    "    def read_prior(self, before_most_recent=0):\n",
    "        path = sorted((g['run_path'] / 'runs').iterdir(), reverse=True)[before_most_recent]\n",
    "        for X in ['bgs_hist', 'stats_hist']:\n",
    "            self[X] = pd.read_parquet(path / f'{X}.parquet')\n",
    "        self['steps'] = np.unique(self['bgs_hist']['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "posted-might",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Texas'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting bgs - found /home/jupyter/simulations/2017/TX/bgs.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create Gerry object\n",
    "# Looks for local saved copies of bgs, pairs, and graph (fast) unless otherwise specified in overwrite\n",
    "# Anything not found is compiled from source (slow)\n",
    "# clr_seq = color scheme ... see last cell for other options\n",
    "# pop_err_max_pct = maximum allowed departure of any cd from perfect population balance\n",
    "\n",
    "# geo_simplification determines how aggressively the polygons are smoothed - changes will not take effect until existing 'bgs' file is overwritten/deleted\n",
    "# min_graph_degree = smallest allowed number of neighbors for each bg; assigns nearest non-adjacent neighbors if not enough - - changes will not take effect until existing 'graph' file is overwritten/deleted\n",
    "# overwrite = which of 'bgs', 'pairs', and 'graph' to overwrite and recompile from source\n",
    "\n",
    "\n",
    "states = get_states()\n",
    "states\n",
    "for i, state in states.iterrows():\n",
    "    if state['abbr'] == 'TX':\n",
    "        display(state['name'])\n",
    "        g = Gerry(abbr=state['abbr'],\n",
    "                  yr=2017,\n",
    "                  clr_seq=px.colors.qualitative.Antique,\n",
    "                  pop_err_max_pct=0.8,\n",
    "                  geo_simplification=0.003,\n",
    "                  min_graph_degree=1,\n",
    "                  overwrite=[],\n",
    "                  seed=30,\n",
    "                 )\n",
    "    #     g.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "worse-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_cols(df):\n",
    "    df.rename(columns={x:x.lower() for x in df.columns}, inplace=True)\n",
    "\n",
    "vtds = gpd.read_file(\"/home/jupyter/TX_VTD/\")\n",
    "lower_cols(vtds)  \n",
    "\n",
    "# blocks = gpd.read_file(\"/home/jupyter/TX_blocks/\", rows=10)\n",
    "# lower_cols(blocks)\n",
    "# blocks.rename(columns={'geoid20' : 'geo_id'}, inplace=True)\n",
    "\n",
    "A=vtds[['cntyvtd','geometry']].set_index('cntyvtd').to_crs(crs_area)\n",
    "A['area']=A.area\n",
    "\n",
    "B = g.bgs[['geo_id', 'geometry', 'total_pop']].set_index('geo_id').to_crs(crs_area)\n",
    "B = B.iloc[:10]\n",
    "#B=blocks[['geo_id','geometry']].set_index('geo_id').to_crs(crs_area)\n",
    "B['area']=B.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "useful-celebrity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cntyvtd\n",
       "10001     5.639817e+06\n",
       "10002     2.572960e+08\n",
       "10003     7.114401e+07\n",
       "10004     2.421842e+08\n",
       "10005     1.696968e+08\n",
       "              ...     \n",
       "410006    2.059615e+08\n",
       "410060    1.296257e+07\n",
       "410061    9.068122e+06\n",
       "410005    1.035158e+08\n",
       "410007    1.613602e+08\n",
       "Name: area, Length: 9190, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#blocks.head(3)\n",
    "g.bgs.head(3)\n",
    "B['area']\n",
    "A['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "musical-turning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_id</th>\n",
       "      <th>cntyvtd</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50022</td>\n",
       "      <td>0.397112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50023</td>\n",
       "      <td>0.418262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50025</td>\n",
       "      <td>0.083107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50029</td>\n",
       "      <td>0.101519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480050001012</td>\n",
       "      <td>50006</td>\n",
       "      <td>0.054413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>480050001012</td>\n",
       "      <td>50022</td>\n",
       "      <td>0.472163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>480050001012</td>\n",
       "      <td>50029</td>\n",
       "      <td>0.473425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>480050001013</td>\n",
       "      <td>50006</td>\n",
       "      <td>0.719140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480050001013</td>\n",
       "      <td>50021</td>\n",
       "      <td>0.169436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480050001013</td>\n",
       "      <td>50029</td>\n",
       "      <td>0.110524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50007</td>\n",
       "      <td>0.797254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50023</td>\n",
       "      <td>0.044852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50024</td>\n",
       "      <td>0.030274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50025</td>\n",
       "      <td>0.127620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50007</td>\n",
       "      <td>0.703879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50008B</td>\n",
       "      <td>0.069482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50015</td>\n",
       "      <td>0.183420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50024</td>\n",
       "      <td>0.043219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>480050002001</td>\n",
       "      <td>50003</td>\n",
       "      <td>0.791402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>480050002001</td>\n",
       "      <td>50004</td>\n",
       "      <td>0.193248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>480050002001</td>\n",
       "      <td>50006</td>\n",
       "      <td>0.011041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>480050002002</td>\n",
       "      <td>50001</td>\n",
       "      <td>0.016641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>480050002002</td>\n",
       "      <td>50003</td>\n",
       "      <td>0.912472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>480050002002</td>\n",
       "      <td>50012</td>\n",
       "      <td>0.064389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>480050002003</td>\n",
       "      <td>50012</td>\n",
       "      <td>0.219128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>480050002003</td>\n",
       "      <td>50035</td>\n",
       "      <td>0.038975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>480050002003</td>\n",
       "      <td>50037</td>\n",
       "      <td>0.729709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>480050003011</td>\n",
       "      <td>50004</td>\n",
       "      <td>0.470162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>480050003011</td>\n",
       "      <td>50021</td>\n",
       "      <td>0.519849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>480050003012</td>\n",
       "      <td>50008B</td>\n",
       "      <td>0.715551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>480050003012</td>\n",
       "      <td>50024</td>\n",
       "      <td>0.282081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          geo_id cntyvtd      area\n",
       "0   480050001011   50022  0.397112\n",
       "1   480050001011   50023  0.418262\n",
       "2   480050001011   50025  0.083107\n",
       "3   480050001011   50029  0.101519\n",
       "4   480050001012   50006  0.054413\n",
       "5   480050001012   50022  0.472163\n",
       "6   480050001012   50029  0.473425\n",
       "7   480050001013   50006  0.719140\n",
       "8   480050001013   50021  0.169436\n",
       "9   480050001013   50029  0.110524\n",
       "10  480050001021   50007  0.797254\n",
       "11  480050001021   50023  0.044852\n",
       "12  480050001021   50024  0.030274\n",
       "13  480050001021   50025  0.127620\n",
       "14  480050001022   50007  0.703879\n",
       "15  480050001022  50008B  0.069482\n",
       "16  480050001022   50015  0.183420\n",
       "17  480050001022   50024  0.043219\n",
       "18  480050002001   50003  0.791402\n",
       "19  480050002001   50004  0.193248\n",
       "20  480050002001   50006  0.011041\n",
       "21  480050002002   50001  0.016641\n",
       "22  480050002002   50003  0.912472\n",
       "23  480050002002   50012  0.064389\n",
       "24  480050002003   50012  0.219128\n",
       "25  480050002003   50035  0.038975\n",
       "26  480050002003   50037  0.729709\n",
       "27  480050003011   50004  0.470162\n",
       "28  480050003011   50021  0.519849\n",
       "29  480050003012  50008B  0.715551\n",
       "30  480050003012   50024  0.282081"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B = B.iloc[:10]\n",
    "def f(bg):\n",
    "    C = A.intersection(bg['geometry']).area\n",
    "    C /= bg['area']\n",
    "    mask = C > 1e-2\n",
    "    return C[mask]\n",
    "\n",
    "d = {geo_id:f(bg) for geo_id, bg in B.iterrows()}\n",
    "I = pd.concat(d).reset_index()\n",
    "I.columns = ['geo_id', 'cntyvtd', 'area']\n",
    "I.to_parquet('/home/jupyter/bg_vtd_proportions.parquet', index=False)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fancy-donna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geo_id\n",
       "480050001011    1100.396185\n",
       "480050001011    1159.005019\n",
       "480050001011     230.289346\n",
       "480050001011     281.309434\n",
       "480050001012      77.266095\n",
       "480050001012     670.470771\n",
       "480050001012     672.263091\n",
       "480050001013    1519.541780\n",
       "480050001013     358.018967\n",
       "480050001013     233.536434\n",
       "480050001021    3183.434942\n",
       "480050001021     179.092778\n",
       "480050001021     120.885285\n",
       "480050001021     509.585855\n",
       "480050001022     631.379818\n",
       "480050001022      62.325010\n",
       "480050001022     164.527649\n",
       "480050001022      38.767509\n",
       "480050002001    1979.295764\n",
       "480050002001     483.312927\n",
       "480050002001      27.613520\n",
       "480050002002      34.896450\n",
       "480050002002    1913.453427\n",
       "480050002002     135.023272\n",
       "480050002003     543.657703\n",
       "480050002003      96.698057\n",
       "480050002003    1810.406974\n",
       "480050003011     959.131229\n",
       "480050003011    1060.492519\n",
       "480050003012    1051.859674\n",
       "480050003012     414.659675\n",
       "dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = I[['geo_id','area','cntyvtd']].set_index('geo_id')\n",
    "P = (L['area']/B['area'])*B['total_pop']\n",
    "VTD_pop = L['area']*B['total_pop'] \n",
    "VTD_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eleven-immigration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_id</th>\n",
       "      <th>cntyvtd</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50022</td>\n",
       "      <td>0.397112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50023</td>\n",
       "      <td>0.418262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50025</td>\n",
       "      <td>0.083107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480050001011</td>\n",
       "      <td>50029</td>\n",
       "      <td>0.101519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480050001012</td>\n",
       "      <td>50006</td>\n",
       "      <td>0.054413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>480050001012</td>\n",
       "      <td>50022</td>\n",
       "      <td>0.472163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>480050001012</td>\n",
       "      <td>50029</td>\n",
       "      <td>0.473425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>480050001013</td>\n",
       "      <td>50006</td>\n",
       "      <td>0.719140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480050001013</td>\n",
       "      <td>50021</td>\n",
       "      <td>0.169436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480050001013</td>\n",
       "      <td>50029</td>\n",
       "      <td>0.110524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50007</td>\n",
       "      <td>0.797254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50023</td>\n",
       "      <td>0.044852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50024</td>\n",
       "      <td>0.030274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>480050001021</td>\n",
       "      <td>50025</td>\n",
       "      <td>0.127620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50007</td>\n",
       "      <td>0.703879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50008B</td>\n",
       "      <td>0.069482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50015</td>\n",
       "      <td>0.183420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>480050001022</td>\n",
       "      <td>50024</td>\n",
       "      <td>0.043219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>480050002001</td>\n",
       "      <td>50003</td>\n",
       "      <td>0.791402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>480050002001</td>\n",
       "      <td>50004</td>\n",
       "      <td>0.193248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>480050002001</td>\n",
       "      <td>50006</td>\n",
       "      <td>0.011041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>480050002002</td>\n",
       "      <td>50001</td>\n",
       "      <td>0.016641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>480050002002</td>\n",
       "      <td>50003</td>\n",
       "      <td>0.912472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>480050002002</td>\n",
       "      <td>50012</td>\n",
       "      <td>0.064389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>480050002003</td>\n",
       "      <td>50012</td>\n",
       "      <td>0.219128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>480050002003</td>\n",
       "      <td>50035</td>\n",
       "      <td>0.038975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>480050002003</td>\n",
       "      <td>50037</td>\n",
       "      <td>0.729709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>480050003011</td>\n",
       "      <td>50004</td>\n",
       "      <td>0.470162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>480050003011</td>\n",
       "      <td>50021</td>\n",
       "      <td>0.519849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>480050003012</td>\n",
       "      <td>50008B</td>\n",
       "      <td>0.715551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>480050003012</td>\n",
       "      <td>50024</td>\n",
       "      <td>0.282081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          geo_id cntyvtd      area\n",
       "0   480050001011   50022  0.397112\n",
       "1   480050001011   50023  0.418262\n",
       "2   480050001011   50025  0.083107\n",
       "3   480050001011   50029  0.101519\n",
       "4   480050001012   50006  0.054413\n",
       "5   480050001012   50022  0.472163\n",
       "6   480050001012   50029  0.473425\n",
       "7   480050001013   50006  0.719140\n",
       "8   480050001013   50021  0.169436\n",
       "9   480050001013   50029  0.110524\n",
       "10  480050001021   50007  0.797254\n",
       "11  480050001021   50023  0.044852\n",
       "12  480050001021   50024  0.030274\n",
       "13  480050001021   50025  0.127620\n",
       "14  480050001022   50007  0.703879\n",
       "15  480050001022  50008B  0.069482\n",
       "16  480050001022   50015  0.183420\n",
       "17  480050001022   50024  0.043219\n",
       "18  480050002001   50003  0.791402\n",
       "19  480050002001   50004  0.193248\n",
       "20  480050002001   50006  0.011041\n",
       "21  480050002002   50001  0.016641\n",
       "22  480050002002   50003  0.912472\n",
       "23  480050002002   50012  0.064389\n",
       "24  480050002003   50012  0.219128\n",
       "25  480050002003   50035  0.038975\n",
       "26  480050002003   50037  0.729709\n",
       "27  480050003011   50004  0.470162\n",
       "28  480050003011   50021  0.519849\n",
       "29  480050003012  50008B  0.715551\n",
       "30  480050003012   50024  0.282081"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.draw_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run recom MCMC for specified number of steps, reporting progress every \"update_period\" steps\n",
    "# results save to timestamped file in /home/jupyter/simulations/yr/state/runs\n",
    "steps = 4\n",
    "g.run_mcmc(steps=steps, update_period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved MCMC runs\n",
    "# \"before_most_recent\" = how far back to go .. 0=most recent run, 1=run before that, 2=run before that, ...\n",
    "g.read_prior(before_most_recent=0)\n",
    "for step in g['steps'][::1]:\n",
    "    g.draw_map(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.colors.qualitative.swatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-religious",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
